UnitFlo Evaluation Project
Overview
The UnitFlo Evaluation Project is a robust, production-ready solution designed to evaluate the quality of unit tests generated by the unitflo.ai VS Code extension. It uses the UnitFlo Evaluation Metric (UFEM) to measure the effectiveness of the generated tests, with a focus on Python codebases that involve complex logic, API interactions, machine learning models, and data processing.

Features
Complex Python Codebase: The project contains complex Python modules with real-world edge cases.
UFEM Evaluation Metric: A composite metric that assesses various aspects of test quality, such as code coverage, correctness, edge case handling, and more.
Automated Testing: Pre-configured test suite using pytest and other testing tools.
Mocking and Dependency Injection: Simulate external APIs, services, and data sources to isolate the code under test.
Docker Support: Dockerfile included to ensure consistent environments across different machines.
Continuous Integration: Pre-configured GitHub Actions workflow to automate testing, linting, and code quality checks.
Repository Structure
markdown
Copy code
unitflo-evaluation-project/
├── .github/
│   └── workflows/
│       └── ci.yml
├── .gitignore
├── .pre-commit-config.yaml
├── CONTRIBUTING.md
├── Dockerfile
├── LICENSE
├── README.md
├── requirements.txt
├── setup.cfg
├── src/
│   ├── __init__.py
│   ├── api_client.py
│   ├── config/
│   │   ├── __init__.py
│   │   └── settings.py
│   ├── data/
│   │   ├── __init__.py
│   │   └── data_source.py
│   ├── data_processor.py
│   ├── exceptions.py
│   ├── model.py
│   ├── services/
│   │   ├── __init__.py
│   │   ├── service_a.py
│   │   └── service_b.py
│   ├── utilities.py
│   └── validations.py
├── tests/
│   ├── __init__.py
│   ├── test_api_client.py
│   ├── test_data_processor.py
│   ├── test_model.py
│   ├── test_services.py
│   ├── test_utilities.py
│   └── test_validations.py
├── tests_evaluation/
│   ├── __init__.py
│   └── test_evaluate_tests.py
├── edge_cases/
│   └── jira_edge_cases.json
├── embeddings/
│   └── code_embeddings.json
├── evaluate_tests.py
├── config_evaluation.yml
└── utils/
    ├── __init__.py
    ├── llm_utils.py
    └── logger.py
Setup Instructions
1. Clone the Repository
bash
Copy code
git clone https://github.com/yourusername/unitflo-evaluation-project.git
cd unitflo-evaluation-project
(Replace yourusername with your GitHub username once you upload the repository.)

2. Install Dependencies
It's recommended to use a virtual environment.

bash
Copy code
python3 -m venv venv
source venv/bin/activate  # On Windows, use venv\Scripts\activate
pip install -r requirements.txt
3. Set Up Environment Variables
Create a .env file in the root directory or set environment variables directly in your shell.

bash
Copy code
export OPENAI_API_KEY='your-openai-api-key'
export API_ENDPOINT='https://api.example.com'
export API_USERNAME='your-api-username'
export API_PASSWORD='your-api-password'
export TIMEOUT='30'
Note: Ensure you have the appropriate API keys and credentials.

4. Configure Settings
The config/settings.py file manages configuration settings.

python
Copy code
# src/config/settings.py
import os

class Settings:
    API_ENDPOINT = os.getenv('API_ENDPOINT', 'https://api.example.com')
    API_USERNAME = os.getenv('API_USERNAME', 'user')
    API_PASSWORD = os.getenv('API_PASSWORD', 'pass')
    TIMEOUT = int(os.getenv('TIMEOUT', '30'))

    if TIMEOUT <= 0:
        raise ValueError("TIMEOUT must be a positive integer")
Generating Unit Tests with unitflo.ai
1. Open the Project in VS Code
bash
Copy code
code .
2. Use the unitflo.ai Extension
Ensure the unitflo.ai extension is installed and configured in VS Code.
Generate unit tests for all modules in the src/ directory.
The extension should leverage:
Edge Cases: Use edge_cases/jira_edge_cases.json.
Codebase Index: Utilize embeddings from embeddings/code_embeddings.json.
Full Codebase Files: Access all source files in src/.
3. Save Generated Tests
Save all generated test files in the tests/ directory.
Use a consistent naming convention, e.g., test_data_processor.py.
Evaluating the Generated Tests
1. Run the Evaluation Script
bash
Copy code
python evaluate_tests.py
2. Understand the Output
The script will output component scores and the final UnitFlo Evaluation Metric (UFEM) score.

Example:

java
Copy code
Evaluating tests using UFEM...
Code Coverage Score (CCS): 85.00%
Test Correctness Score (TCS): 80.00%
Edge Case Handling Score (ECHS): 75.00%
Test Quality Score (TQS): 90.00%
Exception Handling Score (EHS): 80.00%
Duplication and Redundancy Score (DRS): 95.00%
Execution Success Rate (ESR): 100.00%

Final UFEM Score: 84.25%
Project Components
1. Source Code Files (src/)
data_processor.py: Handles complex data transformations and processes unknown data values.
api_client.py: Manages API interactions and authentication.
model.py: Defines a machine learning model with dynamic imports and predictions.
utilities.py: Utility functions for normalization, outlier detection, and input processing.
validations.py: Validation functions for data integrity checks.
exceptions.py: Custom exceptions for error handling.
config/settings.py: Configuration settings for API credentials and timeouts.
2. Test Suite (tests/)
Contains unit tests for all source code modules, leveraging mocking to isolate units and simulate external dependencies.
pytest is used as the test runner, with coverage reports generated using pytest-cov.
3. Evaluation Script (evaluate_tests.py)
Evaluates the generated unit tests using the UnitFlo Evaluation Metric (UFEM).
The script assesses:
Code Coverage (CCS)
Test Correctness (TCS)
Edge Case Handling (ECHS)
Test Quality (TQS)
Exception Handling (EHS)
Duplication and Redundancy (DRS)
Execution Success Rate (ESR)
4. Configuration Files
config_evaluation.yml: External configuration for the evaluation script, including weights for the UFEM components and linter settings.
5. Edge Cases and Embeddings
Edge Cases: Defined in edge_cases/jira_edge_cases.json, containing scenarios the generated tests should cover.
Code Embeddings: Stored in embeddings/code_embeddings.json, providing additional context for the LLM.
6. Docker Support
Dockerfile: Builds a Docker image to ensure consistent environments for running the evaluation.
Testing
Running the Test Suite
bash
Copy code
pytest --cov=src --cov-report=term-missing
This command will run all unit tests and display a test coverage report.

Pre-Commit Hooks
Pre-commit hooks are configured to enforce code quality checks using tools like flake8, black, and pylint.

Code Quality
Linters and Formatters
The project uses:

flake8 for linting.
black for code formatting.
pylint for additional static code analysis.
These are configured in the setup.cfg file and automatically enforced via pre-commit hooks.

Continuous Integration
The project includes a GitHub Actions workflow configured in .github/workflows/ci.yml. This workflow:

Installs dependencies.
Runs pre-commit hooks.
Executes the test suite.
Runs the UFEM evaluation script.
This ensures automated testing and code quality checks on each pull request or commit to the main branch.

Docker Usage
To run the project inside a Docker container:

bash
Copy code
docker build -t unitflo-evaluation .
docker run -it unitflo-evaluation
This ensures consistency across environments, making it easier to run the evaluation on any machine.

Contributing
We welcome contributions to the project! Please see CONTRIBUTING.md for guidelines on how to contribute.

License
This project is licensed under the MIT License. See the LICENSE file for details.

Contact
For questions or support, please contact the development team at support@unitflo.ai.

Acknowledgments
Special thanks to the contributors and open-source projects that made this project possible.

Feel free to reach out if you need any assistance!